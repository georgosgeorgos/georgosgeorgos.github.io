<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link href="https://fonts.cdnfonts.com/css/ubuntu-mono" rel="stylesheet">
    <link rel="icon" href="./assets/me.png">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        a {
            color: #4a6741;
            text-decoration: none;
            /* font-weight: bold; */
        }

        a:focus,
        a:hover {
            color: #4a6741;
            text-decoration: none;
            /* font-weight: bold; */
        }

        body,
        td,
        th,
        tr,
        p,
        a

        /* {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        } */
            {
            font-family: 'Ubuntu', monospace;
            font-size: 14px;
        }

        strong {
            font-family: 'Ubuntu', monospace;
            font-size: 14px;
        }

        /* {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        } */

        heading {
            font-family: 'Ubuntu', monospace;
            font-size: 22px;
        }

        /* {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
        } */

        papertitle {
            font-family: 'Ubuntu', monospace;
            font-size: 14px;
            font-weight: 700
        }

        /* {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: 700
        } */

        name {
            font-family: 'Ubuntu', monospace;
            font-size: 32px;
        }

        /* {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        } */

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }

        .new {
            background-color: #cc0000;
            color: white;
            border-radius: 5px;
            padding: 1px;
            font-size: 14px;
            margin: 0 5px;
        }
    </style>
    <link rel="icon" type="image/png" href="http://www.linkedin.com/in/giorgio-c-giannone/">

    <title>gcg</title>

    <link href="./css" rel="stylesheet" type="text/css">

</head>

<body data-gr-c-s-loaded="true" style="background-color: white">
    <table width="840" border="0" align="center" cellspacing="0" cellpadding="0">
        <tbody>
            <tr>
                <td>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td width="65%" valign="middle">
                                    <p align="left">
                                        <name>Giorgio Giannone</name>
                                    </p>
                                    
                                    <p align="left">
                                        <!-- <a href="mailto:gigi%3Cat%3Edtu%3Cdot%3Edk" target="_blank">Email</a> &nbsp;/&nbsp; -->
                                        <!-- <a href="mailto:giorgio%3Cdot%3Ec%3Cdot%3Egiannone%3Cat%3Egmail%3Cdot%3Ecom" target="_blank">Email</a> &nbsp;/&nbsp; -->
                                        <a href="./assets/cv/giorgio_giannone_cv.pdf"
                                            target="_blank"><strong>CV</strong></a>
                                        &nbsp;/&nbsp;
                                        <a href="https://github.com/georgosgeorgos"
                                            target="_blank"><strong>GitHub</strong></a>
                                        &nbsp;/&nbsp;
                                        <a href="http://www.linkedin.com/in/giorgio-c-giannone/" target="_blank">
                                            <strong>LinkedIn</strong></a> &nbsp;/&nbsp;
                                        <a href="https://scholar.google.com/citations?user=1qsJQhkAAAAJ&hl=en"
                                            target="_blank"><strong>Scholar</strong></a>
                                        <br><br>
                                    </p>
                                    <!-- ######## ABSTRACT #########-->
                                                                       
                                    <p>
                                        I am a <a>Principal Research Scientist</a> on the AI Innovation Team at <a href="https://ai-innovation.team" target="_blank"><strong>Red Hat</strong></a> in Boston, 
                                        where I lead research on Probabilistic Inference Methods for Foundation Models. 
                                        <br><br>
                                        In collaboration with the Core AI Division at IBM, I develop Context Optimization algorithms to improve the reliability and cost efficiency of enterprise-scale 
                                        <a href="https://www.ibm.com/think/topics/agentops" target="_blank">AgentOps</a>.
                                        
                                        <br><br>
                                        I also hold an appointment as a <a>Research Affiliate</a> with the <a href="https://decode.mit.edu" target="_blank"><strong>DeCoDE Lab</strong></a> at <a href="https://meche.mit.edu/" target="_blank"><strong>MIT MechE</strong></a>, 
                                        where I contribute to research on Constrained Generative Models for AI-Driven Design and Program Synthesis.
                                    </p> 
                                    
                                    <p align="center">
                                        <a target="_blank"><img width="0%"></a>
                                        <a href="https://www.redhat.com/en" target="_blank"><img
                                                src="./assets/logos/Red_Hat_logo.png" alt="blind-date" width="25%"></a>
                                        <a target="_blank"><img width="0%"></a>
                                    </p>

                                    <p align="center">
                                        <a href="https://www.dima.uniroma1.it/dima/en"
                                            href="http://datascience.i3s.uniroma1.it/it" target="_blank"><img
                                                src="./assets/logos/sapienza_logo.jpeg" alt="blind-date" width="7%"></a>
                                        <a href="https://engineering.nyu.edu" target="_blank"><img
                                                src="./assets/logos/nyu_logo.jpeg" alt="blind-date" width="7%"></a>
                                        <a href="https://www.compute.dtu.dk" target="_blank"><img
                                                src="./assets/logos/dtu_logo_red.jpeg" alt="blind-date" width="7%"></a>
                                        <a href="https://engineering.mit.edu" target="_blank"><img
                                                src="./assets/logos/mit_logo.jpeg" alt="blind-date" width="7%"></a>
                                        <a href="https://www.ucl.ac.uk/ai-centre/" target="_blank"><img
                                                src="./assets/logos/ucl-logo.png" alt="blind-date" width="7%"></a>
                                        <a href="https://www.amazon.science/locations/seattle-and-bellevue"
                                            target="_blank"><img src="./assets/logos/amazon.png"
                                                alt="blind-date" width="7%"></a>
                                        <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/"
                                            target="_blank"><img src="./assets/logos/msft_logo.png" alt="blind-date"
                                                width="7%"></a>
                                        <a href="https://www.zurich.ibm.com" target="_blank"><img
                                                src="./assets/logos/ibm_logo.jpeg" alt="blind-date" width="7%"></a>
                                        <a href="https://www.amazon.science" target="_blank"><img
                                                src="./assets/logos/amazon_logo.jpeg" alt="blind-date" width="7%"></a>
                                        <a href="https://nnaisense.com" target="_blank"><img
                                                src="./assets/logos/nnaisense_logo.jpeg" alt="blind-date"
                                                width="7%"></a>
                                        <a href="https://europe.naverlabs.com" target="_blank"><img
                                                src="./assets/logos/naver_labs_logo.jpeg" alt="blind-date"
                                                width="7%"></a>
                                    <br><br>
                                    </p>

                                    <p>    
                                        Previously, I was an <a>Applied Scientist</a> at <a
                                            href="https://www.amazon.science/locations/seattle-and-bellevue"
                                            target="_blank"><strong>Amazon</strong></a> in Seattle, where I worked on
                                        grounding Vision-Language Models in product-centric contexts
                                        and enhancing the fidelity of Subject-Driven Text-to-Image Synthesis.
                                        <!-- </b>  -->
                                        <br><br>
                                        I completed my PhD at the Technical
                                        University of Denmark (<a
                                            href="https://www.compute.dtu.dk/english/research/research-sections/cogsys"
                                            target="_blank"><strong>DTU</strong></a>), supervised by <a
                                            href="https://olewinther.github.io" target="_blank">Ole Winther</a>
                                        and <a href="https://www2.compute.dtu.dk/~sohau/" target="_blank">S&oslash;ren
                                            Hauberg</a>.
                                        <br><br>   
                                        During my PhD, I was a visiting researcher at <a
                                            href="https://engineering.mit.edu" target="_blank"><strong>MIT</strong></a>
                                        School of Engineering
                                        and <a href="https://www.ucl.ac.uk/ai-centre/"
                                            target="_blank"><strong>UCL</strong></a> Centre for Artificial Intelligence,
                                        collaborated with the <a href="https://mitibmwatsonailab.mit.edu"
                                            target="_blank"><strong>MIT-IBM AI Lab</strong></a>,
                                        and interned at <a
                                            href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/"
                                            target="_blank"><strong>Microsoft Research</strong></a> (Cambridge, MA),
                                        <a href="https://research.ibm.com/labs/zurich" target="_blank"><strong>IBM
                                                Research</strong></a> (Zurich, CH),
                                        and <a href="https://www.amazon.science/locations/london-and-cambridge"
                                            target="_blank"><strong>Amazon Science</strong></a> (Cambridge and London,
                                        UK).
                                    </p>
                                    

                                    <!-- <p align="center">
                                        <a target="_blank"><img width="0%"></a>
                                        <a href="https://www.redhat.com/en" target="_blank"><img
                                                src="./assets/logos/Red_Hat_logo.png" alt="blind-date" width="25%"></a>
                                        <a target="_blank"><img width="0%"></a>
                                    </p>

                                    <p align="center">
                                        <a href="https://www.dima.uniroma1.it/dima/en"
                                            href="http://datascience.i3s.uniroma1.it/it" target="_blank"><img
                                                src="./assets/logos/sapienza_logo.jpeg" alt="blind-date" width="7%"></a>
                                        <a href="https://engineering.nyu.edu" target="_blank"><img
                                                src="./assets/logos/nyu_logo.jpeg" alt="blind-date" width="7%"></a>
                                        <a href="https://www.compute.dtu.dk" target="_blank"><img
                                                src="./assets/logos/dtu_logo_red.jpeg" alt="blind-date" width="7%"></a>
                                        <a href="https://engineering.mit.edu" target="_blank"><img
                                                src="./assets/logos/mit_logo.jpeg" alt="blind-date" width="7%"></a>
                                        <a href="https://www.ucl.ac.uk/ai-centre/" target="_blank"><img
                                                src="./assets/logos/ucl-logo.png" alt="blind-date" width="7%"></a>
                                        <a href="https://www.amazon.science/locations/seattle-and-bellevue"
                                            target="_blank"><img src="./assets/logos/amazon.png"
                                                alt="blind-date" width="7%"></a>
                                        <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/"
                                            target="_blank"><img src="./assets/logos/msft_logo.png" alt="blind-date"
                                                width="7%"></a>
                                        <a href="https://www.zurich.ibm.com" target="_blank"><img
                                                src="./assets/logos/ibm_logo.jpeg" alt="blind-date" width="7%"></a>
                                        <a href="https://www.amazon.science" target="_blank"><img
                                                src="./assets/logos/amazon_logo.jpeg" alt="blind-date" width="7%"></a>
                                        <a href="https://nnaisense.com" target="_blank"><img
                                                src="./assets/logos/nnaisense_logo.jpeg" alt="blind-date"
                                                width="7%"></a>
                                        <a href="https://europe.naverlabs.com" target="_blank"><img
                                                src="./assets/logos/naver_labs_logo.jpeg" alt="blind-date"
                                                width="7%"></a>
                                    </p> -->
                                    <!-- <br> -->
                                    <!-- <p align="center">
                                        <a href="mailto:gigi%3Cat%3Edtu%3Cdot%3Edk" target="_blank">Email</a> &nbsp;/&nbsp; -->
                                        <!-- <a href="mailto:giorgio%3Cdot%3Ec%3Cdot%3Egiannone%3Cat%3Egmail%3Cdot%3Ecom" target="_blank">Email</a> &nbsp;/&nbsp; -->
                                        <!-- <a href="./assets/cv/giorgio_giannone_cv.pdf"
                                            target="_blank"><strong>CV</strong></a>
                                        &nbsp;/&nbsp;
                                        <a href="https://github.com/georgosgeorgos"
                                            target="_blank"><strong>GitHub</strong></a>
                                        &nbsp;/&nbsp;
                                        <a href="http://www.linkedin.com/in/giorgio-c-giannone/" target="_blank">
                                            <strong>LinkedIn</strong></a> &nbsp;/&nbsp;
                                        <a href="https://scholar.google.com/citations?user=1qsJQhkAAAAJ&hl=en"
                                            target="_blank"><strong>Scholar</strong></a>

                                    </p> -->
                                </td>

                                <td width="25%" valign="top">
                                    <a href="http://www.linkedin.com/in/giorgio-c-giannone/" target="_blank"><img
                                            src="./assets/me2.jpg" width="80%"
                                            style="border-radius: 80px 80px 80px 80px; margin-top: 60px;"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td width="100%" valign="middle">
                                    <p>
                                        <em>
                                            I am a researcher and engineer working on Generative AI and
                                            Probabilistic Methods, <br>
                                            with a focus on Inference-Time Scaling, Test-Time Adaptation, Vision-Language Alignment, and Conditional
                                            Diffusion Models.
                                        </em>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table> -->

                    <!-- RESEARCH -->
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td width="100%" valign="middle">
                                    <heading id="research">Research</heading>

                                    <p>
                                            I am a researcher and engineer working on Generative AI and
                                            Probabilistic Methods, <br>
                                            with a focus on Inference-Time Scaling, Test-Time Adaptation, Vision-Language Alignment, and Conditional
                                            Diffusion Models.
                                        <br><br>
                                        I am interested in the generalization and adaptation capabilities of
                                        hierarchical generative models, with a focus on feedback-driven self-training
                                        and vision-language alignment. I aim to bridge the gap between large and small
                                        generative models by leveraging probabilistic inference techniques and bayesian methods.
                                    </p>

                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- ######## PAPERS #########-->


                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

                        <tbody>
                            <tr>
                                <td>
                                    <heading>Publications</heading>
                                </td>

                            </tr>
                        </tbody>

                        <tbody>

                            <tr>
                                <td width="25%"><img src="./assets/images/epf.png" alt="blind-date" width="90%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/2510.05825" target="_blank">
                                            <papertitle>Mitigating Premature Exploitation in Particle-based Monte Carlo for Inference-Time Scaling
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://gxxu-ml.github.io/" target="_blank">Guangxuan Xu</a>,
                                        <a href="https://www.linkedin.com/in/nikhil-na" target="_blank">Nikhil Shivakumar Nayak</a>,
                                        <a href="https://rohanawhad.com/" target="_blank">Rohan Mahesh Awhad</a>,
                                        <a href="https://www.linkedin.com/in/shivchanders" target="_blank">Shivchander Sudalairaj</a>,
                                        <a href="https://xuk.ai/" target="_blank">Kai Xu</a>,
                                        <a href="https://akashgit.github.io" target="_blank">Akash Srivastava</a>
                                        <br>
                                        <em>Under Review</em>, 2025
                                        <br>
                                    </p>
                                    <p>
                                        Inference-Time Scaling (ITS) improves language models by allocating more computation at generation time. 
                                        Particle Filtering (PF) has emerged as a strong ITS method for complex mathematical reasoning tasks, 
                                        but it is vulnerable when guided by process reward models, which often assign overconfident scores early in the reasoning process. 
                                        This causes PF to suffer from premature exploitation: it myopically commits to locally promising trajectories, 
                                        prunes potentially correct hypotheses, and converges to suboptimal solutions. This failure mode, known as particle impoverishment, 
                                        is especially severe under constrained computational budgets. 
                                        To address this, we analyze the problem and identify two root causes: a lack of diversity in the particle set due to 
                                        overconfident resampling and consequent inability to assess the potential of a reasoning path. 
                                        We introduce Entropic Particle Filtering (ePF), an algorithm that integrates two new techniques to solve these issues. 
                                        <!-- The first technique, Entropic Annealing (EA), directly mitigates particle impoverishment by monitoring search diversity via entropy; 
                                        when diversity drops, it intervenes by dynamically annealing the resampling distribution to preserve exploration. 
                                        The second, an enhancement called Look-ahead Modulation (LaM), adds a predictive guide to evaluate a state's potential based 
                                        on its successors. -->
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/svp.png" alt="blind-date" width="90%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/2501.04568" target="_blank">
                                            <papertitle>Feedback-Driven Vision-Language Alignment with Minimal Human
                                                Supervision
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://liruoteng.github.io" target="_blank">Ruoteng Li</a>,
                                        <a href="https://www.amazon.science/author/qianli-feng" target="_blank">Qianli
                                            Feng</a>,
                                        <a href="https://www.amazon.science/author/yev-perevodchikov"
                                            target="_blank">Yev Perevodchikov</a>,
                                        <a href="https://www.amazon.science/author/rui-chen" target="_blank">Rui
                                            Chen</a>,
                                        <a href="https://www.linkedin.com/in/aleix-martinez-480110162"
                                            target="_blank">Aleix Martinez</a>
                                        <br>
                                        <em>Under Review</em>, 2025
                                        <br>
                                    </p>
                                    <p>
                                        Vision-language models (VLMs) have demonstrated remarkable potential in
                                        integrating visual and linguistic information, but their performance is often
                                        constrained by the need for extensive, high-quality image-text training data.
                                        Curation of these image-text pairs is both time-consuming and computationally
                                        expensive. To address this challenge, we introduce SVP (Supervision-free Visual
                                        Projection), a novel framework that enhances vision-language alignment without
                                        relying on curated data or preference annotation. SVP leverages self-captioning
                                        and a pre-trained grounding model as a feedback mechanism to elicit latent
                                        information in VLMs.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/olip.png" alt="blind-date" width="80%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Be_More_Specific_Evaluating_Object-centric_Realism_in_Synthetic_Images_CVPR_2025_paper.html" target="_blank">
                                            <papertitle>Be More Specific: Evaluating Object-centric Realism in Synthetic
                                                Images
                                            </papertitle>
                                        </a>
                                        <br>
                                        <a href="https://www.amazon.science/author/anqi-liang" target="_blank">Anqi
                                            Liang</a>,
                                        <a href="https://www.amazon.science/author/ciprian-corneanu"
                                            target="_blank">Ciprian Corneanu</a>,
                                        <a href="https://www.amazon.science/author/qianli-feng" target="_blank">Qianli
                                            Feng</a>,
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://www.linkedin.com/in/aleix-martinez-480110162"
                                            target="_blank">Aleix Martinez</a>
                                        <br>
                                        <em>Conference on Computer Vision and Pattern Recognition, CVPR</em>, 2025
                                        <br>
                                    </p>
                                    <p>
                                        Evaluation of synthetic images is important for both model development and
                                        selection.
                                        An ideal evaluation should be specific, accurate and aligned with human
                                        perception.
                                        This paper addresses the problem of evaluating realism of objects in synthetic
                                        images.
                                        Although methods has been proposed to evaluate holistic realism,
                                        there are no methods tailored towards object-centric realism evaluation.
                                        In this work, we define a new standard for assessing object-centric realism that
                                        follows a shape-texture breakdown
                                        and proposes the first object-centric realism evaluation dataset for synthetic
                                        images.
                                        The dataset contains images generated from state-of-the-art image generative
                                        models
                                        and is richly annotated at object level across a diverse set of object
                                        categories.
                                        We then design and train the OLIP model, a dedicated architecture that
                                        considerably outperforms
                                        any existing baseline on object-centric realism evaluation.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/mrconv_icon.png" alt="blind-date" width="90%">
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/2408.09453" target="_blank">
                                            <papertitle>Reparameterized Multi-Resolution Convolutions for Long Sequence
                                                Modelling
                                            </papertitle>
                                        </a>
                                        <br>
                                        <a href="https://hjakecunningham.github.io" target="_blank">Harry Jake
                                            Cunningham</a>,
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://mingtian.ai" target="_blank">Mingtian Zhang</a>,
                                        <a href="https://www.deisenroth.cc" target="_blank">Marc Deisenroth</a>
                                        <br>
                                        <em>Neural Information Processing Systems,
                                            NeurIPS</em>, 2024
                                        <br>
                                    </p>
                                    <p>
                                        Global convolutions have shown increasing promise as powerful general-purpose
                                        sequence models. However, training long convolutions is challenging, and kernel
                                        parameterizations must be able to learn long-range dependencies without
                                        overfitting. This work introduces reparameterized multi-resolution convolutions
                                        (MRConv),
                                        a novel approach to parameterizing global convolutional kernels for
                                        long-sequence modeling. By leveraging multi-resolution convolutions,
                                        incorporating structural reparameterization and introducing learnable kernel
                                        decay, MRConv learns expressive long-range kernels that perform well across
                                        various data
                                        modalities.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/nito.png" alt="blind-date" width="90%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/2402.05073" target="_blank">
                                            <papertitle>NITO: Neural Implicit Fields for Resolution-free Topology
                                                Optimization
                                            </papertitle>
                                        </a>
                                        <br>
                                        <a href="https://engineering.mit.edu/fellows/amin-heyrani-nobari/"
                                            target="_blank">Amin Heyrani Nobari</a>,
                                        <a href="https://scholar.google.com/citations?user=MQ9nKDsAAAAJ&hl=en"
                                            target="_blank">Lyle Regenwetter</a>,
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://meche.mit.edu/people/faculty/faez@MIT.EDU" target="_blank">Faez
                                            Ahmed</a>
                                        <br>
                                        <em>Transactions on Machine Learning Research, TMLR</em>, 2025
                                        <br>
                                    </p>
                                    <p>
                                        Topology optimization is a critical task in engineering design, where the goal
                                        is to optimally distribute material in a given space for maximum performance.
                                        We introduce Neural Implicit Topology Optimization (NITO), a novel approach to
                                        accelerate topology optimization problems using deep learning.
                                        NITO stands out as one of the first frameworks to offer a resolution-free and
                                        domain-agnostic solution in deep learning-based topology optimization.
                                        NITO synthesizes structures with up to seven times better structural efficiency
                                        compared to SOTA diffusion models and does so in a tenth of the time.
                                        In the NITO framework, we introduce a novel method, the Boundary Point
                                        Order-Invariant MLP (BPOM), to represent boundary conditions in a sparse and
                                        domain-agnostic manner, moving away from expensive simulation-based approaches.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/ndgm.png" alt="blind-date" width="90%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="" target="_blank">
                                            <papertitle>Constraining Generative Models for Engineering Design with
                                                Negative Data
                                            </papertitle>
                                        </a>
                                        <br>
                                        <a href="https://scholar.google.com/citations?user=MQ9nKDsAAAAJ&hl=en"
                                            target="_blank">Lyle Regenwetter</a>,
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://akashgit.github.io" target="_blank">Akash Srivastava</a>,
                                        <a href="https://research.ibm.com/people/dan-gutfreund" target="_blank">Dan
                                            Gutfreund</a>,
                                        <a href="https://meche.mit.edu/people/faculty/faez@MIT.EDU" target="_blank">Faez
                                            Ahmed</a>
                                        <br>
                                        <em>Transactions on Machine Learning Research, TMLR</em>, 2024
                                        <br>
                                    </p>
                                    <p>
                                        Generative models have recently achieved remarkable success and widespread
                                        adoption in society, yet they still often struggle to generate realistic and
                                        accurate outputs.
                                        This challenge extends beyond language and vision into fields like engineering
                                        design, where safety-critical engineering standards and
                                        non-negotiable physical laws tightly constrain what outputs are considered
                                        acceptable.
                                        In this work, we introduce two approaches to guide models toward
                                        constraint-satisfying outputs using negative data -- examples of what to avoid.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/concept_manufacturing_gpt4v.png"
                                        alt="blind-date" width="90%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://link.springer.com/article/10.1007/s10462-025-11290-y" target="_blank">
                                            <papertitle>From Concept to Manufacturing: Evaluating Vision-Language Models
                                                for Engineering Design <a href="https://www.youtube.com/watch?v=WL2fqiK7zlE" target="_blank">[talk]</a>
                                            </papertitle>
                                        </a>
                                        <br>
                                        <a href="https://meche.mit.edu/people/staff/cyrilp@mit.edu">Cyril Picard</a>*,
                                        <a href="https://orcid.org/0009-0000-4316-8119">Kristen M. Edwards</a>*,
                                        <a href="https://orcid.org/0009-0003-0125-0909">Anna C. Doris</a>,
                                        <a href="https://orcid.org/0009-0002-9892-430X">Brandon Man</a>,
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://orcid.org/0000-0002-8469-7591">Md Ferdous Alam</a>,
                                        <a href="https://meche.mit.edu/people/faculty/faez@MIT.EDU">Faez Ahmed</a>
                                        <br>
                                        <em>Artificial Intelligence Review</em>, 2025
                                        <br>
                                    </p>
                                    Engineering Design is undergoing a transformative shift with the advent of AI,
                                    marking a new era in how we approach product, system, and service planning.
                                    Large language models have demonstrated impressive capabilities in enabling this
                                    shift.
                                    Yet, with text as their only input modality, they cannot leverage the large body of
                                    visual artifacts that engineers have used for centuries and are accustomed to.
                                    This gap is addressed with the release of multimodal vision language models, such as
                                    GPT-4V, enabling AI to impact many more types of tasks.
                                    In light of these advancements, this paper presents a comprehensive evaluation of
                                    GPT-4V, a vision language model, across a wide spectrum of engineering design tasks,
                                    categorized into four main areas: Conceptual Design, System-Level and Detailed
                                    Design, Manufacturing and Inspection, and Engineering Education Tasks.
                                    <p>

                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/ta.png" alt="blind-date" width="90%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/2305.18470" target="_blank">
                                            <papertitle>Aligning Optimization Trajectories with Diffusion Models for
                                                Constrained Design Generation
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://akashgit.github.io" target="_blank">Akash Srivastava</a>,
                                        <a href="https://olewinther.github.io/" target="_blank">Ole Winther</a>,
                                        <a href="https://meche.mit.edu/people/faculty/faez@MIT.EDU" target="_blank">Faez
                                            Ahmed</a>
                                        <br>
                                        <em>Neural Information Processing Systems, NeurIPS</em>, 2023
                                        <br>
                                    </p>
                                    <p>
                                        Generative models have had a profound impact on vision and language, paving the
                                        way for a new era of multimodal generative applications.
                                        While these successes have inspired researchers to explore using generative
                                        models in science and engineering to accelerate the design process and reduce
                                        the reliance on iterative optimization, challenges remain.
                                        Specifically, engineering optimization methods based on physics still outperform
                                        generative models when dealing with constrained environments where data is
                                        scarce and precision is paramount.
                                        To address these challenges, we introduce Diffusion Optimization Models (DOM)
                                        and Trajectory Alignment (TA),
                                        a learning framework that demonstrates the efficacy of aligning the sampling
                                        trajectory of diffusion models with the optimization trajectory derived from
                                        traditional physics-based methods.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/invalid2valid.png" alt="blind-date"
                                        width="90%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://openreview.net/forum?id=73wnK2BvWg" target="_blank">
                                            <papertitle>Improving Precision in Language Models Learning from Invalid
                                                Samples
                                            </papertitle>
                                        </a>
                                        <br>
                                        <a href="https://github.com/jakob949">Niels Jakob Larsen</a>*,
                                        <strong>Giorgio Giannone</strong>*,
                                        <a href="https://olewinther.github.io/" target="_blank">Ole Winther</a>,
                                        <a href="https://orbit.dtu.dk/en/persons/kai-kristof-blin" target="_blank">Kai
                                            Blin</a>
                                        <br>
                                        <em>Generative AI and Biology Workshop, NeurIPS</em>, 2023
                                        <br>
                                    </p>
                                    Language Models are powerful generative tools capable of learning intricate patterns
                                    from vast amounts of unstructured data. Nevertheless, in domains that demand
                                    precision, such as science and engineering, the primary objective is to obtain an
                                    exact and accurate answer. Precision takes precedence in these contexts. In
                                    specialized tasks like chemical compound generation, the emphasis is on output
                                    accuracy rather than response diversity. Traditional self-refinement methods are
                                    ineffective for such domain-specific input/output pairs, unlike general language
                                    tasks. In this study, we introduce invalid2valid, a powerful and general
                                    post-processing mechanism that can significantly enhance precision in language
                                    models for input/output tasks spanning different domains and specialized
                                    applications.
                                    <p>

                                    </p>
                                </td>
                            </tr>


                            <tr>
                                <td width="25%"><img src="./assets/images/align_adapt_llm_bio.png" alt="blind-date"
                                        width="90%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://openreview.net/forum?id=SJYY3nQ0cX" target="_blank">
                                            <papertitle>Enhancing Language Models for Technical Domains with Dynamic
                                                Token Injection
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://www.microsoft.com/en-us/research/people/netenenh/"
                                            target="_blank">Neil Tenenholtz</a>,
                                        <a href="https://www.microsoft.com/en-us/research/people/jamhall/"
                                            target="_blank">James Hall</a>,
                                        <a href="http://nicolofusi.com" target="_blank">Nicolo Fusi</a>,
                                        <a href="https://dmelis.github.io" target="_blank">David Alvarez-Melis</a>
                                        <br>
                                        <em>Generative AI and Biology Workshop, NeurIPS</em>, 2023
                                        <br>
                                    </p>
                                    Large language models (LLMs) are rapidly advancing the frontier of natural language
                                    understanding and generation. Their generalist nature, while adept at handling a
                                    wide range of tasks, often lacks the depth and precision required by highly
                                    specialized and rapidly evolving technical domains, such as genomics and engineering
                                    design. Fine-tuning these models for specific domains can be effective but requires
                                    large amounts of data and compromises their general reasoning capabilities. In this
                                    work, we introduce a scalable method to infuse specialized knowledge into generalist
                                    language models by dynamically extending their vocabulary with specialist tokens. By
                                    using a lightweight functional mapping on an extended vocabulary and adjusting the
                                    logit distribution, we enable the model to grasp domain-specific nuances.
                                    <p>

                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/csgm.png" alt="blind-date" width="90%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/2306.15166" target="_blank">
                                            <papertitle>Learning from Invalid Data: On Constraint Satisfaction in
                                                Generative Models
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>*,
                                        <a href="https://scholar.google.com/citations?user=MQ9nKDsAAAAJ&hl=en"
                                            target="_blank">Lyle Regenwetter*</a>,
                                        <a href="https://akashgit.github.io" target="_blank">Akash Srivastava*</a>,
                                        <a href="https://research.ibm.com/people/dan-gutfreund" target="_blank">Dan
                                            Gutfreund</a>,
                                        <a href="https://meche.mit.edu/people/faculty/faez@MIT.EDU" target="_blank">Faez
                                            Ahmed</a>
                                        <br>
                                        <em>Diffusion Models Workshop, NeurIPS</em>, 2023
                                        <br>
                                    </p>
                                    <p>
                                        Generative models have demonstrated impressive results in vision, language, and
                                        speech.
                                        However, even with massive datasets, they struggle with precision, generating
                                        physically invalid or factually incorrect data.
                                        This is particularly problematic when the generated data must satisfy
                                        constraints, for example, to meet product specifications in engineering design
                                        or to adhere to the laws of physics in a natural scene.
                                        To improve precision while preserving diversity and fidelity, we propose a novel
                                        training mechanism that leverages datasets of constraint-violating data points,
                                        which we consider invalid.
                                    </p>
                                </td>
                            </tr>


                            <tr>
                                <td width="25%"><img src="./assets/images/mdmt-clm.png" alt="blind-date" width="90%">
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/2301.12586" target="_blank">
                                            <papertitle>Unifying Molecular and Textual Representations via Multi-task
                                                Language Modelling
                                            </papertitle>
                                        </a>
                                        <br>
                                        <a href="https://researcher.watson.ibm.com/researcher/view.php?person=zurich-DIC"
                                            target="_blank">Dimitrios Christofidellis</a>*,
                                        <strong>Giorgio Giannone</strong>*,
                                        <a href="https://research.ibm.com/people/jannis-born" target="_blank">Jannis
                                            Born</a>,
                                        <a href="https://olewinther.github.io/" target="_blank">Ole Winther</a>,
                                        <a href="https://research.ibm.com/people/teodoro-laino" target="_blank">Teodoro
                                            Laino</a>,
                                        <a href="https://research.ibm.com/people/matteo-manica" target="_blank">Matteo
                                            Manica</a>
                                        <br>
                                        <em>International Conference on Machine Learning, ICML</em>, 2023
                                        <br>
                                    </p>
                                    <p>
                                        The recent advances in neural language models have also been successfully
                                        applied to the field of chemistry,
                                        offering generative solutions for classical problems in molecular design and
                                        synthesis planning. These new methods have the potential to optimize laboratory
                                        operations and fuel
                                        a new era of data-driven automation in scientific discovery. However,
                                        specialized models are still typically required for each task, leading to the
                                        need for problem-specific
                                        fine-tuning and neglecting task interrelations. Here, we propose a
                                        multi-domain, multi-task language model to solve a wide range of tasks in both
                                        the chemical and natural
                                        language domains. By leveraging multi-task learning, our model can handle
                                        chemical and natural language concurrently, without requiring expensive
                                        pre-training on single
                                        domains or task-specific models.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/todm.png" alt="blind-date" width="90%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/2303.09760" target="_blank">
                                            <papertitle>Diffusing the Optimal Topology: A Generative Optimization
                                                Approach
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://meche.mit.edu/people/faculty/faez@MIT.EDU" target="_blank">Faez
                                            Ahmed</a>
                                        <br>
                                        <em>International Design Engineering Technical Conferences, IDETC</em>, 2023
                                        <br>
                                    </p>
                                    <p>
                                        Topology Optimization seeks to find the best design that satisfies a set of
                                        constraints while maximizing system performance.
                                        Traditional iterative optimization methods like SIMP can be computationally
                                        expensive and get stuck in local minima,
                                        limiting their applicability to complex or large-scale problems.
                                        Recently, deep generative models, such as Generative Adversarial
                                        Networks and Diffusion Models,
                                        conditioned on constraints and physics fields have shown promise, but they
                                        require extensive pre-processing and surrogate models for improving performance.
                                        To address these issues, we propose a Generative Optimization method that
                                        integrates classic optimization like SIMP as a refining mechanism for the
                                        topology generated by a deep generative model.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/gt4sd_npj.png" alt="blind-date" width="70%">
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://www.nature.com/articles/s41524-023-01028-1" target="_blank">
                                            <papertitle>Accelerating material design with the generative toolkit for
                                                scientific discovery
                                            </papertitle>
                                        </a>
                                        <br>
                                        <a href="https://gt4sd.github.io/gt4sd-core/" target="_blank">Manica & the GT4SD
                                            Team (Core Contributor)</a>
                                        <br>
                                        <em>Nature npj Computational Materials</em>, 2023
                                        <br>
                                    </p>
                                    With the growing availability of data within various scientific domains,
                                    generative models hold enormous potential to accelerate scientific discovery.
                                    They harness powerful representations learned from datasets to speed up the
                                    formulation
                                    of novel hypotheses with the potential to impact material discovery broadly.
                                    We present the Generative Toolkit for Scientific Discovery (GT4SD).
                                    This extensible open-source library enables scientists, developers,
                                    and researchers to train and use state-of-the-art generative models
                                    to accelerate scientific discovery focused on organic material design.
                                    <p>

                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/fsdm.png" alt="blind-date" width="70%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/2205.15463" target="_blank">
                                            <papertitle>Few-Shot Diffusion Models
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://didriknielsen.github.io/" target="_blank">Didrik Nielsen</a>,
                                        <a href="https://olewinther.github.io/" target="_blank">Ole Winther</a>
                                        <br>
                                        <em>Score-Based Methods Workshop, NeurIPS</em>, 2022
                                        <br>
                                    </p>
                                    <p>
                                        Denoising diffusion probabilistic models (DDPM) are powerful hierarchical latent
                                        variable models with remarkable sample generation quality and training
                                        stability.
                                        These properties can be attributed to parameter sharing in the generative
                                        hierarchy, as well as a parameter-free diffusion-based inference procedure.
                                        In this paper, we present Few-Shot Diffusion Models (FSDM), a framework for
                                        few-shot generation leveraging conditional DDPMs.
                                        FSDMs are trained to adapt the generative process conditioned on a small set of
                                        images from a given class by aggregating image patch information using a
                                        set-based Vision Transformer (ViT).
                                        At test time, the model is able to generate samples from previously unseen
                                        classes conditioned on as few as 5 samples from that class.
                                        We empirically show that FSDM can perform few-shot generation and transfer to
                                        new datasets taking full advantage of the conditional DDPM.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/scha-vae.png" alt="blind-date" width="90%">
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/2110.12279" target="_blank">
                                            <papertitle>SCHA-VAE: Hierarchical Context Aggregation for
                                                Few-Shot Generation
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://olewinther.github.io/" target="_blank">Ole Winther</a>
                                        <br>
                                        <em>International Conference on Machine Learning, ICML</em>, 2022
                                        <br>
                                    </p>
                                    <p>A few-shot generative model should be able to generate data from a novel
                                        distribution by only observing a limited set of examples.
                                        In few-shot learning the model is trained on data from many sets from
                                        distributions sharing some underlying properties such
                                        as sets of characters from different alphabets or objects from different
                                        categories.
                                        We extend current latent variable models for sets to a fully hierarchical
                                        approach with an attention-based point to
                                        set-level aggregation and call our method SCHA-VAE for
                                        Set-Context-Hierarchical-Aggregation Variational Autoencoder.
                                        We explore likelihood-based model comparison, iterative data sampling, and
                                        adaptation-free out-of-distribution generalization.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/jm1_bar.png" alt="blind-date" width="90%">
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/2210.12195" target="_blank">
                                            <papertitle>Just Mix Once: Worst-group Generalization by Group
                                                Interpolation
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://serhii-havrylov.github.io/" target="_blank">Serhii
                                            Havrylov</a>,
                                        <a href="https://uk.linkedin.com/in/jordan-massiah-562862136"
                                            target="_blank">Jordan Massiah</a>,
                                        <a href="https://sites.google.com/site/emineyilmaz/home" target="_blank">Emine
                                            Yilmaz</a>,
                                        <a href="https://yunlongjiao.github.io/" target="_blank">Yunlong Jiao</a>
                                        <br>
                                        <em>Distribution Shifts Workshop, NeurIPS</em>, 2021
                                        <br>
                                    </p>
                                    <p>
                                        Advances in deep learning theory have revealed how average generalization relies
                                        on superficial patterns in data.
                                        The consequences are brittle models with poor performance with shift in group
                                        distribution at test time.
                                        When group annotation is available, we can use robust optimization tools to
                                        tackle the problem.
                                        However, identification and annotation are time-consuming, especially on large
                                        datasets.
                                        A recent line of work leverages self-supervision and oversampling to improve
                                        generalization on minority groups
                                        without group annotation. We propose to unify and generalize these approaches
                                        using a class-conditional variant
                                        of mixup tailored for worst-group generalization. Our approach, Just Mix Once
                                        (JM1),
                                        interpolates samples during learning, augmenting the training distribution with
                                        a continuous mixture of groups.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/hfsgm.png" alt="blind-date" width="90%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://openreview.net/forum?id=INSai0E0VXN" target="_blank">
                                            <papertitle>Hierarchical Few-Shot Generative Models
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://olewinther.github.io/" target="_blank">Ole Winther</a>
                                        <br>
                                        <em>Meta-Learning Workshop, NeurIPS</em>, 2021
                                        <br>
                                    </p>
                                    <p>A few-shot generative model should be able to generate data from a distribution
                                        by only observing a limited set of examples. In few-shot learning the model is
                                        trained on data from many sets from different distributions sharing some
                                        underlying
                                        properties such as sets of characters from different alphabets or sets of images
                                        of different type objects. We study a latent variables approach that extends the
                                        Neural Statistician to a fully hierarchical approach with an attention-based
                                        point to set-level aggregation. We extend the previous work to iterative data
                                        sampling,
                                        likelihood-based model comparison, and adaptation-free out of distribution
                                        generalization.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/gm_all.png" alt="blind-date" width="90%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/1912.03845v2" target="_blank">
                                            <papertitle>Transformation-aware Variational Autoencoder
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://simons.berkeley.edu/people/saeed-saremi" target="_blank">Saeed
                                            Saremi</a>,
                                        <a href="https://people.lu.usi.ch/mascij/" target="_blank">Jonathan Masci</a>,
                                        <a href="https://osdf.github.io/" target="_blank">Christian Osendorfer</a>
                                        <br>
                                        <em>Technical Report</em>, 2020
                                        <br>
                                    </p>
                                    <p>We extend the framework of variational autoencoders to represent transformations
                                        explicitly in the latent space. This is achieved in the form of a generative
                                        model
                                        structured such that the group of transformations
                                        that act in the input space is instead represented by latent variables
                                        which are linear operators that only act in the latent space.
                                        In the family of hierarchical graphical models that emerges,
                                        the latent space is populated by higher order objects which are inferred
                                        jointly with the latent representations they act on.</p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/pipeline.png" alt="blind-date" width="90%">
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/2004.03156" target="_blank">
                                            <papertitle>Real-time Classification from Short Event-Camera Streams using
                                                Input-filtering Neural ODEs
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="http://ashaanoosheh.com/" target="_blank">Asha Anoosheh</a>,
                                        <a href="https://www.linkedin.com/in/aqua83/?originalSubdomain=ch"
                                            target="_blank">Alessio Quaglino</a>,
                                        <a href="https://proceduralia.github.io/" target="_blank">Pierluca D'Oro</a>,
                                        <a href="http://marcogallieri.micso.it/Home.html" target="_blank">Marco
                                            Gallieri</a>,
                                        <a href="https://people.lu.usi.ch/mascij/" target="_blank">Jonathan Masci</a>
                                        <br>
                                        <em>Interpretable Inductive Biases
                                            and Physically Structured Learning Workshop, NeurIPS</em>, 2020
                                        <br>
                                    </p>
                                    <p>Event-based cameras are novel, efficient sensors inspired by the human vision
                                        system,
                                        generating an asynchronous, pixel-wise stream of data.
                                        Learning from such data is generally performed through heavy preprocessing and
                                        event integration into images.
                                        This requires buffering of possibly long sequences and can limit the response
                                        time of the inference system.
                                        In this work, we instead propose to directly use events from a DVS camera,
                                        a stream of intensity changes and their spatial coordinates.
                                        This sequence is used as the input for a novel asynchronous RNN-like
                                        architecture,
                                        the Input-filtering Neural ODEs.</p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/knn.png" alt="blind-date" height="100"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/1912.03845v1" target="_blank">
                                            <papertitle>No Representation without Transformation
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://people.lu.usi.ch/mascij/" target="_blank">Jonathan Masci</a>,
                                        <a href="https://osdf.github.io/" target="_blank">Christian Osendorfer</a>
                                        <br>
                                        <em>Bayesian Deep Learning and Perception as Generative Reasoning Workshops,
                                            NeurIPS </em>, 2019
                                        <br>
                                    </p>
                                    <p>We propose to extend Latent Variable Models with a simple idea:
                                        learn to encode not only samples but also transformations of such samples.
                                        This means that the latent space is not only populated by embeddings
                                        but also by higher order objects that map between these embeddings.
                                        We show how a hierarchical graphical model can be utilized to enforce
                                        desirable algebraic properties of such latent mappings.</p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/common.png" alt="blind-date" width="90%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://arxiv.org/abs/1812.06873" target="_blank">
                                            <papertitle>Learning Common Representation from RGB and Depth Images
                                            </papertitle>
                                        </a>
                                        <br>
                                        <strong>Giorgio Giannone</strong>,
                                        <a href="https://europe.naverlabs.com/people_user/Boris-Chidlovskii/"
                                            target="_blank">Boris Chidlovskii</a>
                                        <br>
                                        <em>Multimodal Learning and Applications Workshop, CVPR</em>, 2019
                                        <br>
                                    </p>
                                    <p>We propose a new deep learning architecture for the tasks of semantic
                                        segmentation
                                        and depth prediction from RGB-D images.
                                        We revise the state of art based on the RGB and depth feature fusion,
                                        where both modalities are assumed to be available at train and test time.
                                        We propose a new architecture where the feature fusion is replaced with a common
                                        deep representation.
                                        Combined with an encoder-decoder type of the network, the architecture can
                                        jointly learn models
                                        for semantic segmentation and depth estimation based on their common
                                        representation.</p>
                                </td>
                            </tr>
                        </tbody>

                        <tbody>
                        </tbody>
                    </table>


                    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

                        <tbody>
                            <tr>
                                <td>
                                    <heading>Applied Projects</heading>
                                </td>

                            </tr>
                        </tbody>

                        <tbody>

                            <tr>
                                <td width="40%"><img src="./assets/images/shoppable-example.png" alt="blind-date"
                                        width="100%"></td>
                                <td width="60%" valign="top">
                                    <p>
                                        <a href="" target="_blank">
                                            <papertitle>Visual Shopping Experience at Amazon Scale
                                            </papertitle>
                                        </a>
                                    </p>
                                    During my first six months at Amazon, I designed and built the company's first
                                    generalist detection and grounding framework, achieving full coverage of the Amazon
                                    Catalogue. This innovative system delivered a 25% improvement in mAP compared 
                                    to the previous in-house solution, while reducing memory consumption
                                    by 75% and maintaining a real-time processing speed of 20+ FPS on standard GPU
                                    hardware. In addition, I developed and benchmarked a downstream multimodal retrieval
                                    pipeline. Together, these generalist detection and multimodal retrieval systems have
                                    significantly enhanced the shopping experience, supporting over 50 million iOPS for
                                    the Visual Shoppable Experience in A/B evaluations. Today, the system is
                                    fully integrated into the Shoppable Experience and is widely available across the
                                    Amazon Store worldwide. Before my departure, I successfully deployed six model
                                    variants, ensuring robust and scalable performance for diverse use cases.
                                    <p>

                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="50%"><img src="./assets/images/prompt_gen-1.png" alt="blind-date"
                                        width="100%"></td>
                                <td width="50%" valign="top">
                                    <p>
                                        <a href="" target="_blank">
                                            <papertitle>Multi-Stage Generative Prompt Optimization for Text-to-Image
                                                Synthesis
                                            </papertitle>
                                        </a>
                                    </p>
                                    Multi-stage SFT and DPO to improve prompt generation for text-to-image synthesis.
                                    <p>

                                    </p>
                                </td>
                            </tr>
                        </tbody>
                        <tbody>
                        </tbody>
                    </table> -->


                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Projects</heading>
                                </td>
                            </tr>
                        </tbody>
                        <tbody>

                            <tr>
                                <td width="25%"><img src="./assets/images/text2cad.png" alt="blind-date" width="100%">
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://text2cad.com" target="_blank">
                                            <papertitle>Text2CAD: Democratizing Engineering Design. Prompt by Prompt.
                                            </papertitle>
                                        </a>
                                        <br>
                                        <a href="https://www.text2cad.com/about" target="_blank">
                                            DesignX Team, 2023
                                        </a>
                                        <br>
                                        <br>
                                    </p>
                                    <p>
                                        I co-led a team of engineers and researchers based at MIT and Caltech.
                                        We have developed a generative tool that allows users to create CAD models using
                                        natural language prompts.
                                        Our aim is to revolutionize CAD software by reimagining engineering design for
                                        the next generation of inventors.
                                        The tool is designed to be user-friendly and accessible to non-experts,
                                        enabling a wide range of users to quickly create complex CAD models without the
                                        need for specialized training.
                                    </p>
                                    </p>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/logos/gaia_logo.jpeg" alt="blind-date" width="80%"
                                        style="border-radius: 20px 20px 20px 20px">
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://luissenlabs.com/great-success-for-ai-worklab-gran-finale/"
                                            target="_blank">
                                            <papertitle>GAiA: Chatbots to enhance Workplace Communication
                                            </papertitle>
                                        </a>
                                        <br>
                                        <a href="" target="_blank">
                                            GAiA Team, 2017-2019
                                        </a>
                                        <br>
                                        <br>
                                    </p>
                                    <p>
                                        Co-founder. Chosen from over 100 startups to join the EnLabs Incubator.
                                        Developed platform-agnostic (outlook, slack, telegram) language assistants
                                        (chatbots) designed to streamline enterprise workflows,
                                        such as scheduling large meetings across distributed teams and automating
                                        routine inquiries.
                                    </p>
                                    </p>
                                    </p>
                                </td>
                            </tr>

                            <!-- <tr>
                                <td width="25%"><img src="./assets/images/drone.png" alt="blind-date" width="100%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="" target="_blank">
                                            <papertitle>MixDot
                                            </papertitle>
                                        </a>
                                        <br>
                                        <a href="" target="_blank">
                                            2016
                                        </a>
                                        <br>
                                        <br>
                                    </p>
                                    <p>
                                        Co-founder. Selected among 100+ startups to join the EnLabs Incubator.
                                    </p>
                                    </p>
                                    </p>
                                </td>
                            </tr> -->

                        </tbody>
                        <tbody>
                        </tbody>
                    </table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Open-source</heading>
                                </td>
                            </tr>
                        </tbody>
                        <tbody>

                            <tr>
                                <td width="25%"><img src="./assets/images/its.png" alt="blind-date"
                                        width="100%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://github.com/Red-Hat-AI-Innovation-Team/its_hub" target="_blank">
                                            <papertitle>its-hub: A Python library for inference-time scaling
                                            </papertitle>
                                        </a>
                                        <br>
                                        <a href=""
                                            target="_blank">
                                            AI Innovation Team, Red Hat AI, 2025
                                        </a>
                                        <br>
                                        <br>
                                    </p>
                                    <p>its_hub is a Python library for inference-time scaling of LLMs that provides 
                                        multiple scaling algorithms (Particle Filtering, Best-of-N, Beam Search, Self-Consistency),
                                         an OpenAI-compatible API with Inference-as-a-Service (IaaS),
                                         Async generation with concurrency limits and error handling
                                         and comprehensive benchmarking tools.
                                        </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/gt4sd_overview.png" alt="blind-date"
                                        width="100%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://github.com/GT4SD/gt4sd-core" target="_blank">
                                            <papertitle>GT4SD: Generative Toolkit for Scientific Discovery
                                            </papertitle>
                                        </a>
                                        <br>
                                        <a href="https://github.com/GT4SD/gt4sd-core/graphs/contributors?from=2022-02-06&to=2022-11-24&type=c"
                                            target="_blank">
                                            GT4SD Team, 2022
                                        </a>
                                        <br>
                                        <br>
                                    </p>
                                    <p>The GT4SD (Generative Toolkit for Scientific Discovery) is an open-source
                                        platform to accelerate hypothesis generation in the scientific discovery
                                        process. It provides a library for making state-of-the-art generative AI models
                                        easier to use.</p>
                                </td>
                            </tr>

                        </tbody>
                        <tbody>
                        </tbody>
                    </table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Datasets</heading>
                                </td>
                            </tr>
                        </tbody>
                        <tbody>

                            <tr>
                                <td width="25%"><img src="./assets/images/256_optimized.png" alt="blind-date"
                                        width="100%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="" target="_blank">
                                            <papertitle>2d Topology Optimization
                                            </papertitle>
                                        </a>
                                        <br>
                                    </p>
                                    <p>We built a dataset of optimized topologies and intermediate optimization steps at
                                        low-resolution (64x64) and
                                        high-resolution (256x256) with constraints.
                                    <ul>
                                        <li>50K low-resolution optimized topologies.</li>
                                        <li>60K high-resolution optimizer topologies.</li>
                                        <li>250K low-resolution intermediate steps.</li>
                                        <li>300K high-resolution intermediate steps.</li>
                                    </ul>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/to_higher_3d.png" alt="blind-date"
                                        width="100%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="" target="_blank">
                                            <papertitle>3d Topology Optimization
                                            </papertitle>
                                        </a>
                                        <br>
                                    </p>
                                    <p>We built a multifidelity dataset of 300K optimized topologies with constraints.
                                    <ul>
                                        <li>150K beams.</li>
                                        <li>100K plates.</li>
                                        <li>50K l-shapes.</li>
                                    </ul>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                        <tbody>
                        </tbody>
                    </table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Teaching</heading>
                                </td>
                            </tr>
                        </tbody>
                        <tbody>

                            <tr>
                                <td width="25%"><img src="./assets/images/dtu_teaching.png" alt="blind-date"
                                        width="50%"></td>
                                <td width="75%" valign="top">
                                    <p> &#x2022;
                                        <a href="https://kurser.dtu.dk/course/2020-2021/02456?menulanguage=en"
                                            target="_blank">
                                            <papertitle>Course 02456 Deep learning
                                            </papertitle>
                                        </a>
                                        (Fall 2020)
                                    </p>
                                    <p> &#x2022;
                                        <a href="https://kurser.dtu.dk/course/2021-2022/02477?menulanguage=en"
                                            target="_blank">
                                            <papertitle>Course 02477 Bayesian Machine Learning
                                            </papertitle>
                                        </a>
                                        (Spring 2021)
                                    </p>
                                    <p> &#x2022;
                                        <a href="https://kurser.dtu.dk/course/2021-2022/02460?menulanguage=en"
                                            target="_blank">
                                            <papertitle>Course 02460 Advanced Machine Learning
                                            </papertitle>
                                        </a>
                                        (Spring 2022)
                                    </p>
                                    <p> &#x2022;
                                        <a href="https://kurser.dtu.dk/course/2022-2023/02456?menulanguage=en"
                                            target="_blank">
                                            <papertitle>Course 02456 Deep learning
                                            </papertitle>
                                        </a>
                                        (Fall 2022)
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                        <tbody>
                        </tbody>
                    </table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Patents</heading>
                                </td>
                            </tr>
                        </tbody>
                        <tbody>

                            <tr>
                                <td width="25%"><img src="./assets/images/consistency_ta.png" alt="blind-date"
                                        width="100%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://patents.google.com/patent/US20250292092A1/en" target="_blank">
                                            <papertitle>Generative optimization models for machine learning
                                            </papertitle>
                                        </a>
                                        <br>
                                        Inventors: <strong>Giorgio Giannone</strong> (MIT), Akash Srivastava (IBM), Faez Ahmed (MIT)
                                        <br>
                                        <em>US Patent US20250292092A1</em>, 2025
                                        <br>
                                    </p>
                                    <p></p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/pred_common1.png" alt="blind-date"
                                        width="100%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <a href="https://patents.google.com/patent/US11263756B2/en" target="_blank">
                                            <papertitle>Method and apparatus for semantic segmentation and depth
                                                completion using a convolutional neural network
                                            </papertitle>
                                        </a>
                                        <br>
                                        Inventors: Boris Chidlovskii (NAVER),
                                        <strong>Giorgio Giannone</strong> (NAVER)
                                        <br>
                                        <em>US Patent US11263756B2</em>, 2022
                                        <br>
                                    </p>
                                    <p></p>
                                </td>
                            </tr>

                        </tbody>
                        <tbody>
                        </tbody>
                    </table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Theses</heading>
                                </td>

                            </tr>
                        </tbody>
                        <tbody>

                            <tr>
                                <td width="25%"><img src="./assets/images/meta-learning-rome.png" alt="blind-date"
                                        width="100%"></td>
                                <td width="75%" valign="top">
                                    <p><a href="https://orbit.dtu.dk/en/publications/few-shot-generative-models-learning-generative-models-with-limite"
                                            target="_blank">

                                            <papertitle>Few-Shot Generative Models: Learning Generative Models with
                                                Limited
                                                Data
                                            </papertitle>
                                        </a>

                                        <br>
                                        Giorgio Giannone
                                        <br>
                                        <em>PhD's Thesis, Machine Learning, Technical University of Denmark</em>, 2023
                                        <br>
                                    </p>
                                    <p></p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/pred_common.png" alt="blind-date"
                                        width="100%"></td>
                                <td width="75%" valign="top">
                                    <p>
                                        <papertitle>Learning Common Representation for Scene Understanding
                                        </papertitle>
                                        <br>
                                        Giorgio Giannone
                                        <br>
                                        <em>Master's Thesis, Data Science, Sapienza University of Rome</em>, 2018
                                        <br>
                                    </p>
                                    <p></p>
                                </td>
                            </tr>

                            <tr>
                                <td width="25%"><img src="./assets/images/bubble.png" alt="blind-date" width="100%">
                                </td>
                                <td width="75%" valign="top">
                                    <p>
                                        <papertitle>Bubble Dynamics in Turbulent Shear Flows
                                        </papertitle>
                                        <br>
                                        Giorgio Giannone
                                        <br>
                                        <em>Master's Thesis, Mechanical Engineering, Sapienza University of Rome</em>,
                                        2016
                                        <br>
                                    </p>
                                    <p></p>
                                </td>
                            </tr>
                        </tbody>
                        <tbody>
                        </tbody>
                    </table>

                </td>
            </tr>
        </tbody>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
            <tr>
                <td>
                    <br>
                    <p align="right">
                        <font size="2">
                            <a href="https://people.eecs.berkeley.edu/~barron/" target="_blank">the original</a>
                        </font>
                    </p>
                </td>
            </tr>
        </tbody>
    </table>

</body>

</html>