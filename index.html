<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link rel="icon" href="./assets/me.png">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        a {
            color: #228b22;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body,
        td,
        th,
        tr,
        p,
        a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }

        .new {
            background-color: #cc0000;
            color: white;
            border-radius: 5px;
            padding: 1px;
            font-size: 14px;
            margin: 0 5px;
        }
    </style>

    <link rel="icon" type="image/png" href="">
    <title>Giorgio Giannone</title>

    <link href="./css" rel="stylesheet" type="text/css">

</head>

<body data-gr-c-s-loaded="true">
    <table width="840" border="0" align="center" cellspacing="0" cellpadding="0">
        <tbody>
            <tr>
                <td>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td width="67%" valign="middle">
                                    <p align="center">
                                        <name>Giorgio Giannone</name>
                                    </p>

                                    <!-- ######## ABSTRACT #########-->
                                    <p>I am a PhD student at the <a href="https://www.compute.dtu.dk/english/research/research-sections/cogsys"
                                            target="_blank">Section for Cognitive Systems</a> at the Technical University of Denmark (<a
                                            href="https://www.compute.dtu.dk/english" target="_blank">DTU</a>),
                                        supervised by <a href="https://scholar.google.com/citations?user=7VAwhzUAAAAJ&hl=en" target="_blank">Ole Winther</a> and
                                        <a href="http://www2.compute.dtu.dk/~sohau/" target="_blank">S&oslashren Hauberg</a>.
                                    </p>

                                    <!-- ######## INFO  #########-->

                                    <p align="center">
                                        <!-- <a href="mailto:gigi%3Cat%3Edtu%3Cdot%3Edk" target="_blank">Email</a> &nbsp;/&nbsp; -->
                                        <!-- <a href="mailto:giorgio%3Cdot%3Ec%3Cdot%3Egiannone%3Cat%3Egmail%3Cdot%3Ecom" target="_blank">Email</a> &nbsp;/&nbsp; -->
                                        <a href="./assets/cv/giorgio_giannone_cv.pdf" target="_blank">CV</a> &nbsp;/&nbsp;
                                        <a href="https://scholar.google.com/citations?user=1qsJQhkAAAAJ&hl=en" target="_blank">Scholar</a>
                                        &nbsp;/&nbsp;
                                        <a href="https://github.com/georgosgeorgos" target="_blank">Github</a> &nbsp;/&nbsp;
                                        <a href="http://www.linkedin.com/in/giorgio-c-giannone/" target="_blank"> LinkedIn </a>
                                        <!-- <a href="https://twitter.com/georgosgeorgos" target="_blank">Twitter</a> -->
                                        <!-- <a href="./blog/index.html">Blog</a> -->
                                    </p>
                                </td>
                                <td width="33%">
                                    <img src="./assets/me2.jpg" width="70%">
                                    <!-- <figcaption align="left">The one on my right.</figcaption> -->
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- ######## NEWS  #########-->

                    <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td width="100%" valign="middle">
                                    <heading id="news">News</heading>
                                    <ul>
                                        <li>Aug 2020: I will present a poster at <a href="https://smiles.skoltech.ru/school" target="_blank">SMILES2020</a>.
                            </li>
                            <li>July 2020: I presented a poster at <a href="https://www.eeml.eu/" target="_blank">EEML2020</a>.
                            </li>
                    <li>June 2020: Starting <a href="https://www.compute.dtu.dk/english/research/research-sections/cogsys"
                                                target="_blank">PhD</a>!
                                        </li>
                                        <li>Apr 2020: Extended preprint for <a href="https://arxiv.org/abs/1912.03845" target="_blank">No Representation without
                                                Transformation</a> on arXiv.
                                        </li>
                                        <li>Oct 2019: Paper accepted at <a href="http://bayesiandeeplearning.org/" target="_blank">BDL</a>
                                            and <a href="https://pgr-workshop.github.io/accepted_papers/" target="_blank">PGR</a> workshops at NeurIPS 2019.
                                        </li>
                                        <li>Apr 2019: Paper accepted at <a href="https://mula-workshop.github.io/" target="_blank">MULA</a> workshop at CVPR
                                            2019.
                                        </li>
                                        <li>Jan 2019: Starting an internship at <a href="https://nnaisense.com" target="_blank">NNAISENSE</a>.
                                        </li>
                                        <li>Oct 2018: <a href="http://datascience.i3s.uniroma1.it/it" target="_blank">MSc</a> Graduation!
                                        </li>
                                        <li>Mar 2018: Starting an internship at <a href="https://europe.naverlabs.com/" target="_blank">Naver Labs Europe</a>.
                                        </li>
                                        <li>Aug 2017: Starting a visiting period at <a href="https://vida.engineering.nyu.edu/" target="_blank">New York
                                                University</a>.
                                        </li>
                                        </li>
                                    </ul>
                                </td>
                            </tr>
                        </tbody>
                    </table> -->

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td width="100%" valign="middle">
                                    <p>
                                        <em>
                                        I am broadly interested in probabilistic machine learning, perception and geometry, <br>
                                        with a focus on deep latent variable models, few-shot generation, transfer learning, and diffusion models.
                                    </em>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- RESEARCH -->
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td width="100%" valign="middle">
                                    <heading id="research">Research</heading>
                                    <p>
                                        I am interested in the generalization and adaptation capacities of hierarchical generative models.<br> <br> 
                                        Large generative models trained on millions of data points exhibit emergent adaptation properties like in-context learning, being able to solve novel tasks given a handful of samples. 
                                        However, such adaptation properties emerge only with scale, being absent for middle-size models trained on small datasets, 
                                        a typical scenario in engineering design and scientific discovery, where data collection is expensive and computational constraints are present.<br> <br> 
                                        My research goal is to bridge the gap in adaptation capabilities between large and middle-size generative models. 
                                        Leveraging the tools of hierarchical inference and using the training dataset in such a way as to encourage adaptation at inference time, 
                                        we can learn expressive families of conditional generative processes and perform few-shot generation and transfer learning in the small data regime.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- ######## PAPERS #########-->


                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Papers</heading>
                                </td>

                            </tr>
                        </tbody>
                        <tbody>
                        <tr>
                            <td width="25%"><img src="./assets/images/fsdm.png" alt="blind-date" width="80%"></td>
                            <td width="75%" valign="top">
                                <p>
                                    <a href="https://arxiv.org/abs/2205.15463" target="_blank">
                                        <papertitle>Few-Shot Diffusion Models
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Giorgio Giannone</strong>,
                                    <a href="https://didriknielsen.github.io/" target="_blank">Didrik Nielsen</a>,
                                    <a href="https://olewinther.github.io/" target="_blank">Ole Winther</a>
                                    <br>
                                    <em>under-review</em>, 2022
                                    <br>
                                </p>
                                <p>
                                    Denoising diffusion probabilistic models (DDPM) are powerful hierarchical latent variable models with remarkable sample generation quality and training stability.
                                    These properties can be attributed to parameter sharing in the generative hierarchy, as well as a parameter-free diffusion-based inference procedure.
                                    In this paper, we present Few-Shot Diffusion Models (FSDM), a framework for few-shot generation leveraging conditional DDPMs. 
                                    FSDMs are trained to adapt the generative process conditioned on a small set of images from a given class by aggregating image patch information using a set-based Vision Transformer (ViT). 
                                    At test time, the model is able to generate samples from previously unseen classes conditioned on as few as 5 samples from that class.
                                    We empirically show that FSDM can perform few-shot generation and transfer to new datasets taking full advantage of the conditional DDPM.
                                </p>
                            </td>
                        </tr>

                        <tr>
                            <td width="25%"><img src="./assets/images/scha-vae.png" alt="blind-date" width="90%"></td>
                            <td width="75%" valign="top">
                                <p>
                                    <a href="https://arxiv.org/abs/2110.12279" target="_blank">
                                        <papertitle>SCHA-VAE: Hierarchical Context Aggregation for
                                            Few-Shot Generation
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Giorgio Giannone</strong>,
                                    <a href="https://olewinther.github.io/" target="_blank">Ole Winther</a>
                                    <br>
                                    <em>International Conference on Machine Learning, ICML</em>, 2022
                                    <br>
                                </p>
                                <p>A few-shot generative model should be able to generate data from a novel distribution by only observing a limited set of examples. 
                                    In few-shot learning the model is trained on data from many sets from distributions sharing some underlying properties such 
                                    as sets of characters from different alphabets or objects from different categories.
                                    We extend current latent variable models for sets to a fully hierarchical approach with an attention-based point to 
                                    set-level aggregation and call our method SCHA-VAE for Set-Context-Hierarchical-Aggregation Variational Autoencoder. 
                                    We explore likelihood-based model comparison, iterative data sampling, and adaptation-free out-of-distribution generalization. 
                                    Our results show that the hierarchical formulation better captures the intrinsic variability within the sets in the small data regime.
                                    This work generalizes deep latent variable approaches to few-shot learning, taking a step toward large-scale few-shot generation with 
                                    a formulation that readily works with current state-of-the-art deep generative models.
                                </p>
                            </td>
                        </tr>

                        <tr>
                            <td width="25%"><img src="./assets/images/jm1_bar.png" alt="blind-date" width="90%"></td>
                            <td width="75%" valign="top">
                                <p>
                                    <a href="https://arxiv.org/abs/2210.12195"
                                        target="_blank">
                                        <papertitle>Just Mix Once: Worst-group Generalization by Group
                                            Interpolation
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Giorgio Giannone</strong>,
                                    <a href="https://serhii-havrylov.github.io/" target="_blank">Serhii Havrylov</a>,
                                    <a href="https://uk.linkedin.com/in/jordan-massiah-562862136" target="_blank">Jordan Massiah</a>,
                                    <a href="https://sites.google.com/site/emineyilmaz/home" target="_blank">Emine Yilmaz</a>,
                                    <a href="https://yunlongjiao.github.io/" target="_blank">Yunlong Jiao</a>,
                                    <br>
                                    <em>Under-review</em>, 2022 - <em>also Distribution Shifts Workshop, NeurIPS</em>, 2021
                                    <br>
                                </p>
                                <p>
                                    Advances in deep learning theory have revealed how average generalization relies on superficial patterns in data. 
                                    The consequences are brittle models with poor performance with shift in group distribution at test time. 
                                    When group annotation is available, we can use robust optimization tools to tackle the problem. 
                                    However, identification and annotation are time-consuming, especially on large datasets.
                                    A recent line of work leverages self-supervision and oversampling to improve generalization on minority groups 
                                    without group annotation. We propose to unify and generalize these approaches using a class-conditional variant 
                                    of mixup tailored for worst-group generalization. Our approach, Just Mix Once (JM1), 
                                    interpolates samples during learning, augmenting the training distribution with a continuous mixture of groups. 
                                    JM1 is domain agnostic and computationally efficient, can be used with any level of group annotation,
                                    and performs on par or better than the state-of-the-art on worst-group generalization. 
                                    Additionally, we provide a simple explanation of why JM1 works.
                                </p>
                            </td>
                        </tr>

                        <tr>
                            <td width="25%"><img src="./assets/images/hfsgm.png" alt="blind-date" width="90%"></td>
                            <td width="75%" valign="top">
                                <p>
                                    <a href="https://openreview.net/forum?id=INSai0E0VXN" target="_blank">
                                        <papertitle>Hierarchical Few-Shot Generative Models
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Giorgio Giannone</strong>,
                                    <a href="https://olewinther.github.io/" target="_blank">Ole Winther</a>
                                    <br>
                                    <em>Meta-Learning Workshop, NeurIPS</em>, 2021
                                    <br>
                                </p>
                                <p>A few-shot generative model should be able to generate data from a distribution
                                    by only observing a limited set of examples. In few-shot learning the model is
                                    trained on data from many sets from different distributions sharing some underlying
                                    properties such as sets of characters from different alphabets or sets of images
                                    of different type objects. We study a latent variables approach that extends the
                                    Neural Statistician to a fully hierarchical approach with an attention-based
                                    point to set-level aggregation. We extend the previous work to iterative data sampling, 
                                    likelihood-based model comparison, and adaptation-free out of distribution
                                    generalization. Our results show that the hierarchical formulation better captures
                                    the intrinsic variability within the sets in the small data regime. With this work
                                    we generalize deep latent variable approaches to few-shot learning, taking a step
                                    towards large-scale few-shot generation with a formulation that readily can work
                                    with current state-of-the-art deep generative models.
                                </p>
                            </td>
                        </tr>

                        <tr>
                            <td width="25%"><img src="./assets/images/gm_all.png" alt="blind-date" width="90%"></td>
                            <td width="75%" valign="top">
                                <p>
                                    <!-- <a href="" target="_blank"> -->
                                    <papertitle>Transformation-aware Variational Autoencoder
                                    </papertitle>
                                    <!-- </a> -->
                                    <br>
                                    <strong>Giorgio Giannone</strong>,
                                    <a href="https://simons.berkeley.edu/people/saeed-saremi" target="_blank">Saeed Saremi</a>,
                                    <a href="https://people.lu.usi.ch/mascij/" target="_blank">Jonathan Masci</a>,
                                    <a href="https://osdf.github.io/" target="_blank">Christian Osendorfer</a>
                                    <br>
                                    <em>Technical Report</em>, 2020
                                    <br>
                                </p>
                                <p>We extend the framework of variational autoencoders to represent transformations
                                    explicitly in the latent space. This is achieved in the form of a generative model
                                    structured such that the group of transformations
                                    that act in the input space is instead represented by latent variables
                                    which are linear operators that only act in the latent space.
                                    In the family of hierarchical graphical models that emerges,
                                    the latent space is populated by higher order objects which are inferred
                                    jointly with the latent representations they act on.</p>
                            </td>
                        </tr>

                        <tr>
                            <td width="25%"><img src="./assets/images/pipeline.png" alt="blind-date" width="90%"></td>
                            <td width="75%" valign="top">
                                <p>
                                    <a href="https://arxiv.org/abs/2004.03156" target="_blank">
                                        <papertitle>Real-time Classification from Short Event-Camera Streams using Input-filtering Neural ODEs
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Giorgio Giannone</strong>,
                                    <a href="http://ashaanoosheh.com/" target="_blank">Asha Anoosheh</a>,
                                    <a href="https://www.linkedin.com/in/aqua83/?originalSubdomain=ch" target="_blank">Alessio Quaglino</a>,
                                    <a href="https://proceduralia.github.io/" target="_blank">Pierluca D'Oro</a>,
                                    <a href="http://marcogallieri.micso.it/Home.html" target="_blank">Marco Gallieri</a>,
                                    <a href="https://people.lu.usi.ch/mascij/" target="_blank">Jonathan Masci</a>
                                    <br>
                                    <em>Interpretable Inductive Biases
                                        and Physically Structured Learning Workshop, NeurIPS</em>, 2020
                                    <br>
                                </p>
                                <p>Event-based cameras are novel, efficient sensors inspired by the human vision system,
                                    generating an asynchronous, pixel-wise stream of data.
                                    Learning from such data is generally performed through heavy preprocessing and event integration into images.
                                    This requires buffering of possibly long sequences and can limit the response time of the inference system.
                                    In this work, we instead propose to directly use events from a DVS camera,
                                    a stream of intensity changes and their spatial coordinates.
                                    This sequence is used as the input for a novel asynchronous RNN-like architecture,
                                    the Input-filtering Neural ODEs.</p>
                            </td>
                        </tr>

                        <tr>
                            <td width="25%"><img src="./assets/images/knn.png" alt="blind-date" height="100"></td>
                            <td width="75%" valign="top">
                                <p>
                                    <a href="https://arxiv.org/abs/1912.03845v1" target="_blank">
                                        <papertitle>No Representation without Transformation
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Giorgio Giannone</strong>,
                                    <a href="https://people.lu.usi.ch/mascij/" target="_blank">Jonathan Masci</a>,
                                    <a href="https://osdf.github.io/" target="_blank">Christian Osendorfer</a>
                                    <br>
                                    <em>Bayesian Deep Learning and Perception as Generative Reasoning Workshops, NeurIPS </em>, 2019
                                    <br>
                                </p>
                                <p>We propose to extend Latent Variable Models with a simple idea:
                                    learn to encode not only samples but also transformations of such samples.
                                    This means that the latent space is not only populated by embeddings
                                    but also by higher order objects that map between these embeddings.
                                    We show how a hierarchical graphical model can be utilized to enforce
                                    desirable algebraic properties of such latent mappings.</p>
                            </td>
                        </tr>

                        <tr>
                            <td width="25%"><img src="./assets/images/common.png" alt="blind-date" width="90%"></td>
                            <td width="75%" valign="top">
                                <p>
                                    <a href="https://arxiv.org/abs/1812.06873" target="_blank">
                                        <papertitle>Learning Common Representation from RGB and Depth Images
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Giorgio Giannone</strong>,
                                    <a href="https://europe.naverlabs.com/people_user/Boris-Chidlovskii/" target="_blank">Boris Chidlovskii</a>
                                    <br>
                                    <em>Multimodal Learning and Applications Workshop, CVPR</em>, 2019
                                    <br>
                                </p>
                                <p>We propose a new deep learning architecture for the tasks of semantic segmentation
                                    and depth prediction from RGB-D images.
                                    We revise the state of art based on the RGB and depth feature fusion,
                                    where both modalities are assumed to be available at train and test time.
                                    We propose a new architecture where the feature fusion is replaced with a common deep representation.
                                    Combined with an encoder-decoder type of the network, the architecture can jointly learn models
                                    for semantic segmentation and depth estimation based on their common representation.</p>
                            </td>
                        </tr>
        </tbody>
        <tbody>
        </tbody>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
            <tr>
                <td>
                    <heading>Open-source</heading>
                </td>
            </tr>
        </tbody>
        <tbody>

            <tr>
                <td width="25%"><img src="./assets/images/gt4sd_logo.png" alt="blind-date" width="100%"></td>
                <td width="75%" valign="top">
                    <p>
                        <a href="https://github.com/GT4SD/gt4sd-core" target="_blank">
                        <papertitle>GT4SD: Generative Toolkit for Scientific Discovery
                        </papertitle>
                        </a>
                        <br>
                        GT4SD Team, 2022
                        <br>
                        <br>
                    </p>
                    <p>The GT4SD (Generative Toolkit for Scientific Discovery) is an open-source platform to accelerate hypothesis generation in the scientific discovery process. It provides a library for making state-of-the-art generative AI models easier to use.</p>
                </td>
            </tr>
        </tbody>
        <tbody>
        </tbody>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
            <tr>
                <td>
                    <heading>Patents</heading>
                </td>
            </tr>
        </tbody>
        <tbody>

            <tr>
                <td width="25%"><img src="./assets/images/pred_common1.png" alt="blind-date" width="100%"></td>
                <td width="75%" valign="top">
                    <p>
                        <a href="https://patents.google.com/patent/US11263756B2/en">
                            <papertitle>Method and apparatus for semantic segmentation and depth completion using a convolutional neural network
                            </papertitle>
                        </a>
                        <br>
                        Boris Chidlovskii,
                        Giorgio Giannone
                        <br>
                        <em>US Patent US11263756B2</em>, 2022
                        <br>
                    </p>
                    <p></p>
                </td>
            </tr>
        </tbody>
        <tbody>
        </tbody>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
            <tr>
                <td>
                    <heading>Theses</heading>
                </td>

            </tr>
        </tbody>
        <tbody>

            <tr>
                <td width="25%"><img src="./assets/images/pred_common.png" alt="blind-date" width="100%"></td>
                <td width="75%" valign="top">
                    <p>
                        <papertitle>Learning Common Representation for Scene Understanding
                        </papertitle>
                        <br>
                        Giorgio Giannone
                        <br>
                        <em>Master's Thesis, Data Science, Sapienza University of Rome</em>, 2018
                        <br>
                    </p>
                    <p></p>
                </td>
            </tr>

            <tr>
                <td width="25%"><img src="./assets/images/bubble.png" alt="blind-date" width="100%"></td>
                <td width="75%" valign="top">
                    <p>
                        <papertitle>Bubble Dynamics in Turbulent Shear Flows
                        </papertitle>
                        <br>
                        Giorgio Giannone
                        <br>
                        <em>Master's Thesis, Mechanical Engineering, Sapienza University of Rome</em>, 2016
                        <br>
                    </p>
                    <p></p>
                </td>
            </tr>
        </tbody>
        <tbody>
        </tbody>
    </table>

    </td>
    </tr>
    </tbody>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td>
          <br>
          <p align="right"><font size="2">
            <a href="https://people.eecs.berkeley.edu/~barron/" target="_blank">the original</a>
            </font>
          </p>
          </td>
        </tr>
    </tbody></table>

</body>

</html>



</script>
</td>
</tr>
</tbody>
</table>



</body>

</html>